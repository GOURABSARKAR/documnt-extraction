{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1f1964c4-a0a1-425f-bd38-e888358386b7",
   "metadata": {},
   "source": [
    "#1. Install Necessary Libraries for data extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9c4d358-8cb1-469b-8893-a33f640e2063",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install unstructured==0.11.2\n",
    "!pip install langchain-community==0.0.2\n",
    "!pip install poppler\n",
    "!pip install teseract\n",
    "!pip install libmagic==1.0\n",
    "!pip install \"unstructured[pdf]\"\n",
    "!pip install tika==2.6.0\n",
    "!pip install git+https://github.com/julian-r/python-magic.git | tail -n 1\n",
    "!pip install \"unstructured[all-docs]\"\n",
    "!pip install \"unstructured[pptx]\"\n",
    "!pip install pdfplumber\n",
    "!pip install chardet nltk"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d878472c-f711-4a13-b23b-b6ae818471b6",
   "metadata": {},
   "source": [
    "#2 Extract PDF Content"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "112a82c8-db20-47f6-9a77-23f8dca730c2",
   "metadata": {},
   "source": [
    "##2.1 Extract PDF data with langchain UnstructuredFileLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06aac4e8-0c3c-459e-b99d-089d35b8fcb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.document_loaders import UnstructuredFileLoader\n",
    "# Specify the path to your PDF file\n",
    "pdf_file_path = 'file-path'\n",
    "loader = UnstructuredFileLoader(pdf_file_path)\n",
    "docs = loader.load()\n",
    "print(docs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c58ffce4-7427-4407-84d4-05b014ed4d66",
   "metadata": {},
   "source": [
    "##2.2 Extract PDF Content with pdfPlumber"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35a7d9ba-c2b9-4907-a34c-9e5d2da3e4df",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pdfplumber\n",
    "\n",
    "# Specify the path to your PDF file\n",
    "pdf_path = \"file_path\"\n",
    "\n",
    "# Open the PDF file\n",
    "with pdfplumber.open(pdf_path) as pdf:\n",
    "    # Iterate through each page\n",
    "    for i, page in enumerate(pdf.pages):\n",
    "        # Extract text from the page\n",
    "        text = page.extract_text()     \n",
    "        # Print the page number and text content\n",
    "        print(f\"--- Page {i + 1} ---\")\n",
    "        print(text)\n",
    "        print(\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4391cc41-b4b0-468a-8769-037e06528da4",
   "metadata": {},
   "source": [
    "##2.3 Extract Tables from PDFs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40e06edb-995a-4e0d-ba5f-9ce10c8df0ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pdfplumber\n",
    "\n",
    "# Specify the path to your PDF file\n",
    "pdf_path = \"file-path\"\n",
    "\n",
    "# Open the PDF file\n",
    "with pdfplumber.open(pdf_path) as pdf:\n",
    "    # Iterate through each page\n",
    "    for i, page in enumerate(pdf.pages):\n",
    "        # Extract tables from the page\n",
    "        tables = page.extract_tables()\n",
    "        \n",
    "        # Print the page number and tables\n",
    "        print(f\"--- Page {i + 1} ---\")\n",
    "        for table in tables:\n",
    "            for row in table:\n",
    "                print(row)\n",
    "            print(\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fc1aaa7-fffe-4689-b4d9-7f4a1d84f627",
   "metadata": {},
   "source": [
    "#3 Extract Microsoft Word Document and Powepoint Presentation Extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bdf3052-6154-4c14-828e-55bac92ade6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import parser from tika\n",
    "from tika import parser\n",
    "#File path where Word Document and PPT is stored\n",
    "file_path = \"file_path\"\n",
    "parsed = parser.from_file(file_path)\n",
    "if parsed and parsed[\"content\"]:\n",
    "    print(parsed[\"content\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "561ab2d1-19aa-4268-9c92-600abf182d8a",
   "metadata": {},
   "source": [
    "#4. Extract unstructured files from URLs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "648dd688-9141-4ea7-afc3-8d73007331c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.document_loaders import UnstructuredURLLoader\n",
    "file_url = \"file_url\"\n",
    "#Optional Headers - We can pass necessary headers required for the url\n",
    "headers = {}\n",
    "loader = UnstructuredURLLoader(urls=[file_url],headers=headers)\n",
    "file_content = loader.load()\n",
    "print(file_content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d33df29-4c7a-459a-a1a8-c3900dd63632",
   "metadata": {},
   "source": [
    "#5. Extract Markdown files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bf0e7ce-8791-4c3c-b2af-42d2c6ecbb73",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fill Up needed"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca42fe1a-ebed-49e8-9082-eddf68be5342",
   "metadata": {},
   "source": [
    "#6. Extract Text Files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8362f28c-5e79-413b-b1e1-193f1017713c",
   "metadata": {},
   "outputs": [],
   "source": [
    "##6.1 Import libraries for nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9a4423d-163d-47a9-9c89-653f05fb0e88",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import chardet\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf921fa8-c0f0-478d-ada7-a626387ddc88",
   "metadata": {},
   "outputs": [],
   "source": [
    "##6.2 Read File"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b568944b-f774-499d-bd5b-5ce752b385fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_file(file_path):\n",
    "    encoding = detect_encoding(file_path)\n",
    "    with open(file_path, 'r', encoding=encoding) as f:\n",
    "        return f.read()\n",
    "\n",
    "text_content = read_file(file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e941fb9c-4162-4875-9161-1eb5f6f0efb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "##6.3 Clear noise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e3c6070-a7c0-4b96-88fa-caa1b87efdd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text(text):\n",
    "    # Remove multiple spaces and tabs\n",
    "    text = re.sub(r'\\s+', ' ', text)\n",
    "    \n",
    "    # Remove punctuation and special characters except periods\n",
    "    text = re.sub(r'[^\\w\\s\\.]', '', text)\n",
    "    \n",
    "    # Convert to lowercase\n",
    "    text = text.lower()\n",
    "    \n",
    "    # Strip leading and trailing whitespace\n",
    "    text = text.strip()\n",
    "    \n",
    "    return text\n",
    "\n",
    "# Example usage\n",
    "cleaned_text = clean_text(text_content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a414638-ded7-47db-b6fb-288475a3fc13",
   "metadata": {},
   "outputs": [],
   "source": [
    "##6.4 Remove Stop Words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "272f10ad-5c03-4118-aef5-c3a82716f842",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_stop_words(text):\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    words = word_tokenize(text)\n",
    "    filtered_words = [word for word in words if word not in stop_words]\n",
    "    return ' '.join(filtered_words)\n",
    "\n",
    "# Example usage\n",
    "no_stop_words_text = remove_stop_words(cleaned_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "236f0be3-07f2-45fa-9e53-70a57651f873",
   "metadata": {},
   "outputs": [],
   "source": [
    "##6.5 Lemmatize Text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6aea8d1d-32a9-4183-b023-7f96bcc7c9fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "def lemmatize_text(text):\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    words = word_tokenize(text)\n",
    "    lemmatized_words = [lemmatizer.lemmatize(word) for word in words]\n",
    "    return ' '.join(lemmatized_words)\n",
    "\n",
    "# Example usage\n",
    "lemmatized_text = lemmatize_text(no_stop_words_text)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (docenv)",
   "language": "python",
   "name": "docenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
